/**
 * NerdXLiveAudioScreen.tsx
 * Full-screen audio-only tutoring interface.
 * Uses the existing NerdXLiveButton logic in a dedicated screen.
 */

import React, { useState, useRef, useCallback, useEffect } from 'react';
import {
    StyleSheet,
    View,
    Text,
    TouchableOpacity,
    Alert,
    SafeAreaView,
    StatusBar,
    Animated,
} from 'react-native';
import { Audio, AVPlaybackStatus } from 'expo-av';
import { Ionicons } from '@expo/vector-icons';
import { useNavigation } from '@react-navigation/native';
import { LinearGradient } from 'expo-linear-gradient';

const WS_URL = 'wss://nerdx-voice.onrender.com/ws/nerdx-live';

// TURN-BASED AUDIO BUFFERING - Wait for turnComplete before playing
const MAX_BUFFER_SIZE = 50;
const PLAYBACK_TIMEOUT_MS = 15000;
const MAX_RECORDING_DURATION_MS = 60000;

type ConnectionState = 'idle' | 'connecting' | 'ready' | 'recording' | 'processing' | 'error';

interface AudioTurn {
    chunks: string[];
    isComplete: boolean;
}

const NerdXLiveAudioScreen: React.FC = () => {
    const navigation = useNavigation();
    const [connectionState, setConnectionState] = useState<ConnectionState>('idle');

    const wsRef = useRef<WebSocket | null>(null);
    const recordingRef = useRef<Audio.Recording | null>(null);
    const soundRef = useRef<Audio.Sound | null>(null);
    const currentAudioTurnRef = useRef<AudioTurn | null>(null);
    const isPlayingRef = useRef(false);
    const playbackTimeoutRef = useRef<NodeJS.Timeout | null>(null);
    const recordingStartTimeRef = useRef<number>(0);

    const pulseAnim = useRef(new Animated.Value(1)).current;
    const waveAnim = useRef(new Animated.Value(0)).current;

    const endSessionRef = useRef<() => Promise<void>>();
    const stopRecordingAndSendRef = useRef<() => Promise<void>>();

    useEffect(() => {
        if (connectionState === 'recording') {
            const pulse = Animated.loop(
                Animated.sequence([
                    Animated.timing(pulseAnim, { toValue: 1.2, duration: 500, useNativeDriver: true }),
                    Animated.timing(pulseAnim, { toValue: 1, duration: 500, useNativeDriver: true }),
                ])
            );
            pulse.start();
            return () => pulse.stop();
        } else {
            pulseAnim.setValue(1);
        }
    }, [connectionState, pulseAnim]);

    useEffect(() => {
        if (connectionState === 'processing' || isPlayingRef.current) {
            const wave = Animated.loop(
                Animated.timing(waveAnim, { toValue: 1, duration: 1500, useNativeDriver: true })
            );
            wave.start();
            return () => wave.stop();
        }
    }, [connectionState, waveAnim]);

    const configureAudioMode = useCallback(async (forRecording: boolean) => {
        try {
            await Audio.setAudioModeAsync({
                allowsRecordingIOS: forRecording,
                playsInSilentModeIOS: true,
                staysActiveInBackground: true,
                shouldDuckAndroid: true,
                playThroughEarpieceAndroid: false,
            });
        } catch (error) {
            console.error('Audio mode error:', error);
        }
    }, []);

    const bufferAudioChunk = useCallback((audioData: string) => {
        if (!currentAudioTurnRef.current) {
            currentAudioTurnRef.current = { chunks: [], isComplete: false };
            console.log('Started new audio turn buffering');
        }
        currentAudioTurnRef.current.chunks.push(audioData);
        if (currentAudioTurnRef.current.chunks.length > MAX_BUFFER_SIZE) {
            currentAudioTurnRef.current.chunks = currentAudioTurnRef.current.chunks.slice(-MAX_BUFFER_SIZE);
        }
        console.log('Buffered chunk ' + currentAudioTurnRef.current.chunks.length);
    }, []);

    const playCompleteAudioTurn = useCallback(async (audioTurn: AudioTurn) => {
        if (isPlayingRef.current || audioTurn.chunks.length === 0) {
            console.log('Skipping playback');
            return;
        }

        isPlayingRef.current = true;
        console.log('Playing ' + audioTurn.chunks.length + ' chunks sequentially');
        await configureAudioMode(false);

        for (let i = 0; i < audioTurn.chunks.length; i++) {
            if (!isPlayingRef.current) break;

            const audioUri = 'data:audio/wav;base64,' + audioTurn.chunks[i];
            try {
                if (soundRef.current) {
                    try { await soundRef.current.unloadAsync(); } catch (e) {}
                }

                const { sound } = await Audio.Sound.createAsync(
                    { uri: audioUri },
                    { shouldPlay: true, volume: 1.0 }
                );
                soundRef.current = sound;

                await new Promise<void>((resolve) => {
                    sound.setOnPlaybackStatusUpdate((status: AVPlaybackStatus) => {
                        if (status.isLoaded && status.didJustFinish) {
                            console.log('Chunk ' + (i + 1) + ' finished');
                            resolve();
                        }
                    });
                });
            } catch (error) {
                console.error('Error playing chunk:', error);
            }
        }

        console.log('Audio turn finished');
        isPlayingRef.current = false;
        setConnectionState('ready');
    }, [configureAudioMode]);

    const completeAudioTurn = useCallback(async () => {
        if (!currentAudioTurnRef.current || currentAudioTurnRef.current.isComplete) return;
        if (playbackTimeoutRef.current) {
            clearTimeout(playbackTimeoutRef.current);
            playbackTimeoutRef.current = null;
        }
        currentAudioTurnRef.current.isComplete = true;
        const turn = currentAudioTurnRef.current;
        console.log('Completing turn: ' + turn.chunks.length + ' chunks');
        await playCompleteAudioTurn(turn);
        currentAudioTurnRef.current = null;
    }, [playCompleteAudioTurn]);

    const startPlaybackTimeout = useCallback(() => {
        if (playbackTimeoutRef.current) clearTimeout(playbackTimeoutRef.current);
        playbackTimeoutRef.current = setTimeout(async () => {
            console.warn('Playback timeout - forcing completion');
            if (currentAudioTurnRef.current && !currentAudioTurnRef.current.isComplete) {
                await completeAudioTurn();
            }
        }, PLAYBACK_TIMEOUT_MS);
    }, [completeAudioTurn]);

    const startRecording = useCallback(async () => {
        if (recordingRef.current || !wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;

        try {
            if (soundRef.current) {
                try { await soundRef.current.stopAsync(); } catch (e) {}
            }
            currentAudioTurnRef.current = null;
            if (playbackTimeoutRef.current) {
                clearTimeout(playbackTimeoutRef.current);
                playbackTimeoutRef.current = null;
            }

            await configureAudioMode(true);

            const { recording } = await Audio.Recording.createAsync({
                android: {
                    extension: '.m4a',
                    outputFormat: Audio.AndroidOutputFormat.MPEG_4,
                    audioEncoder: Audio.AndroidAudioEncoder.AAC,
                    sampleRate: 16000,
                    numberOfChannels: 1,
                    bitRate: 128000,
                },
                ios: {
                    extension: '.m4a',
                    audioQuality: Audio.IOSAudioQuality.MEDIUM,
                    sampleRate: 16000,
                    numberOfChannels: 1,
                    bitRate: 128000,
                    outputFormat: Audio.IOSOutputFormat.MPEG4AAC,
                },
                web: { mimeType: 'audio/webm', bitsPerSecond: 128000 },
            });

            recordingRef.current = recording;
            setConnectionState('recording');
            recordingStartTimeRef.current = Date.now();
            setTimeout(() => {
                if (recordingRef.current) stopRecordingAndSendRef.current?.();
            }, MAX_RECORDING_DURATION_MS);
            console.log('Recording started');
        } catch (error) {
            console.error('Recording error:', error);
            Alert.alert('Microphone Error', 'Could not access microphone.');
            setConnectionState('ready');
        }
    }, [configureAudioMode]);

    const stopRecordingAndSend = useCallback(async () => {
        if (!recordingRef.current) return;

        try {
            const uri = recordingRef.current.getURI();
            try { await recordingRef.current.stopAndUnloadAsync(); } catch (e: any) {}
            recordingRef.current = null;
            setConnectionState('processing');

            if (uri && wsRef.current?.readyState === WebSocket.OPEN) {
                const response = await fetch(uri);
                const blob = await response.blob();
                console.log('Sending audio: ' + blob.size + ' bytes');

                const reader = new FileReader();
                reader.onloadend = () => {
                    const base64data = reader.result as string;
                    const base64 = base64data.split(',')[1] || base64data;
                    if (wsRef.current?.readyState === WebSocket.OPEN) {
                        wsRef.current.send(JSON.stringify({ type: 'audio', data: base64 }));
                        console.log('Audio sent');
                        setTimeout(() => {
                            setConnectionState((c) => c === 'processing' ? 'ready' : c);
                        }, 20000);
                    } else {
                        setConnectionState('ready');
                    }
                };
                reader.onerror = () => setConnectionState('ready');
                reader.readAsDataURL(blob);
            } else {
                setConnectionState('ready');
            }
        } catch (error) {
            console.error('Stop recording error:', error);
            setConnectionState('ready');
        }
    }, []);

    stopRecordingAndSendRef.current = stopRecordingAndSend;

    const handleWebSocketMessage = useCallback((event: any) => {
        try {
            const data = JSON.parse(event.data);
            switch (data.type) {
                case 'ready':
                    setConnectionState('ready');
                    configureAudioMode(false);
                    break;
                case 'audio':
                    if (data.data) {
                        console.log('Received audio chunk');
                        bufferAudioChunk(data.data);
                        startPlaybackTimeout();
                        if (connectionState !== 'recording') setConnectionState('processing');
                    }
                    break;
                case 'turnComplete':
                    console.log('Turn complete - playing audio');
                    completeAudioTurn();
                    break;
                case 'interrupted':
                    console.log('Interrupted');
                    currentAudioTurnRef.current = null;
                    if (playbackTimeoutRef.current) {
                        clearTimeout(playbackTimeoutRef.current);
                        playbackTimeoutRef.current = null;
                    }
                    if (soundRef.current) soundRef.current.stopAsync().catch(() => {});
                    isPlayingRef.current = false;
                    if (!recordingRef.current) setTimeout(() => startRecording(), 100);
                    break;
                case 'error':
                    Alert.alert('Error', data.message);
                    endSession();
                    break;
            }
        } catch (error) {
            console.error('Message error:', error);
        }
    }, [bufferAudioChunk, startPlaybackTimeout, completeAudioTurn, configureAudioMode, startRecording, connectionState]);

    const connect = useCallback(async () => {
        if (connectionState !== 'idle') return;
        setConnectionState('connecting');

        try {
            const { granted } = await Audio.requestPermissionsAsync();
            if (!granted) {
                Alert.alert('Permission Required', 'Microphone access needed.');
                setConnectionState('idle');
                return;
            }

            currentAudioTurnRef.current = null;
            isPlayingRef.current = false;
            if (playbackTimeoutRef.current) {
                clearTimeout(playbackTimeoutRef.current);
                playbackTimeoutRef.current = null;
            }

            const ws = new WebSocket(WS_URL);
            wsRef.current = ws;

            ws.onopen = () => console.log('Connected');
            ws.onmessage = (event: any) => {
                try {
                    const data = JSON.parse(event.data);
                    console.log('WS message:', data.type);
                } catch (e) {}
                handleWebSocketMessage(event);
            };
            ws.onerror = () => {
                setConnectionState('error');
                Alert.alert('Connection Error', 'Could not connect.');
            };
            ws.onclose = () => {
                console.log('WebSocket closed');
                if (connectionState !== 'idle') endSession();
            };
        } catch (error) {
            setConnectionState('error');
        }
    }, [connectionState, handleWebSocketMessage]);

    const endSession = useCallback(async () => {
        currentAudioTurnRef.current = null;
        if (playbackTimeoutRef.current) {
            clearTimeout(playbackTimeoutRef.current);
            playbackTimeoutRef.current = null;
        }
        if (recordingRef.current) {
            try { await recordingRef.current.stopAndUnloadAsync(); } catch (e) {}
            recordingRef.current = null;
        }
        if (soundRef.current) {
            try {
                await soundRef.current.stopAsync();
                await soundRef.current.unloadAsync();
            } catch (e) {}
            soundRef.current = null;
        }
        isPlayingRef.current = false;
        if (wsRef.current) {
            try {
                wsRef.current.send(JSON.stringify({ type: 'end' }));
                wsRef.current.close();
            } catch (e) {}
            wsRef.current = null;
        }
        setConnectionState('idle');
    }, []);

    endSessionRef.current = endSession;

    useEffect(() => {
        return () => { endSessionRef.current?.(); };
    }, []);

    const handlePress = useCallback(() => {
        switch (connectionState) {
            case 'idle': connect(); break;
            case 'ready': startRecording(); break;
            case 'recording': stopRecordingAndSend(); break;
            case 'processing': setConnectionState('ready'); break;
            case 'error': connect(); break;
            default: endSession();
        }
    }, [connectionState, connect, startRecording, stopRecordingAndSend, endSession]);

    const getStatusText = () => {
        switch (connectionState) {
            case 'connecting': return 'Connecting...';
            case 'ready': return 'Tap to speak';
            case 'recording': return 'Listening... tap to send';
            case 'processing': return 'NerdX is thinking...';
            case 'error': return 'Error - tap to retry';
            default: return 'Tap to start';
        }
    };

    const getButtonColor = () => {
        switch (connectionState) {
            case 'connecting':
            case 'processing': return '#FF9800';
            case 'ready': return '#4CAF50';
            case 'recording': return '#F44336';
            case 'error': return '#9E9E9E';
            default: return '#6C63FF';
        }
    };

    return (
        <SafeAreaView style={styles.container}>
            <StatusBar barStyle="light-content" />
            <LinearGradient colors={['#1a1a2e', '#16213e', '#0f3460']} style={styles.gradient}>
                <View style={styles.header}>
                    <TouchableOpacity style={styles.backButton} onPress={() => { endSession(); navigation.goBack(); }}>
                        <Ionicons name="arrow-back" size={24} color="#fff" />
                    </TouchableOpacity>
                    <Text style={styles.title}>NerdX Live</Text>
                    <View style={styles.placeholder} />
                </View>

                <View style={styles.content}>
                    <View style={styles.waveContainer}>
                        {[...Array(5)].map((_, i) => (
                            <Animated.View key={i} style={[styles.waveLine, {
                                height: 40 + Math.sin(i) * 20,
                                opacity: connectionState === 'processing' || isPlayingRef.current ? 0.8 : 0.3,
                                transform: [{ scaleY: waveAnim.interpolate({ inputRange: [0, 0.5, 1], outputRange: [1, 1.5 + i * 0.2, 1] }) }]
                            }]} />
                        ))}
                    </View>

                    <Text style={styles.statusText}>{getStatusText()}</Text>

                    <Animated.View style={{ transform: [{ scale: pulseAnim }] }}>
                        <TouchableOpacity style={[styles.mainButton, { backgroundColor: getButtonColor() }]} onPress={handlePress} activeOpacity={0.8}>
                            {connectionState === 'connecting' || connectionState === 'processing' ? (
                                <Ionicons name="sync" size={48} color="#fff" />
                            ) : connectionState === 'recording' ? (
                                <Ionicons name="mic" size={48} color="#fff" />
                            ) : connectionState === 'ready' ? (
                                <Ionicons name="mic-outline" size={48} color="#fff" />
                            ) : (
                                <Ionicons name="chatbubble-ellipses" size={48} color="#fff" />
                            )}
                        </TouchableOpacity>
                    </Animated.View>

                    <Text style={styles.hintText}>
                        {connectionState === 'idle' ? 'Tap to start your conversation'
                            : connectionState === 'ready' ? 'Tap the button to ask a question'
                            : connectionState === 'recording' ? 'Speak clearly, then tap again to send'
                            : connectionState === 'processing' ? 'NerdX is thinking...'
                            : 'Wait for NerdX to respond...'}
                    </Text>
                </View>

                {connectionState !== 'idle' && (
                    <TouchableOpacity style={styles.endButton} onPress={() => { endSession(); navigation.goBack(); }}>
                        <Ionicons name="close-circle" size={24} color="#F44336" />
                        <Text style={styles.endButtonText}>End Session</Text>
                    </TouchableOpacity>
                )}
            </LinearGradient>
        </SafeAreaView>
    );
};

const styles = StyleSheet.create({
    container: { flex: 1, backgroundColor: '#1a1a2e' },
    gradient: { flex: 1 },
    header: { flexDirection: 'row', alignItems: 'center', justifyContent: 'space-between', paddingHorizontal: 16, paddingTop: 16 },
    backButton: { width: 44, height: 44, borderRadius: 22, backgroundColor: 'rgba(255,255,255,0.1)', justifyContent: 'center', alignItems: 'center' },
    title: { color: '#fff', fontSize: 20, fontWeight: '700' },
    placeholder: { width: 44 },
    content: { flex: 1, justifyContent: 'center', alignItems: 'center', paddingHorizontal: 40 },
    waveContainer: { flexDirection: 'row', alignItems: 'center', justifyContent: 'center', height: 80, marginBottom: 40 },
    waveLine: { width: 4, backgroundColor: '#6C63FF', borderRadius: 2, marginHorizontal: 6 },
    statusText: { color: '#fff', fontSize: 24, fontWeight: '600', marginBottom: 40, textAlign: 'center' },
    mainButton: { width: 120, height: 120, borderRadius: 60, justifyContent: 'center', alignItems: 'center', shadowColor: '#6C63FF', shadowOffset: { width: 0, height: 8 }, shadowOpacity: 0.5, shadowRadius: 16, elevation: 12 },
    hintText: { color: 'rgba(255,255,255,0.6)', fontSize: 14, textAlign: 'center', marginTop: 40, lineHeight: 20 },
    endButton: { flexDirection: 'row', alignItems: 'center', justifyContent: 'center', paddingVertical: 16, marginBottom: 20 },
    endButtonText: { color: '#F44336', fontSize: 16, fontWeight: '600', marginLeft: 8 },
});

export default NerdXLiveAudioScreen;

